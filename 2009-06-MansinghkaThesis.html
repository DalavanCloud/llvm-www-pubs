<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
  <link rel="stylesheet" href="../llvm.css" type="text/css" media="screen" />
  <title>Natively Probabilistic Computation</title>
</head>
<body>

<div class="pub_title">
  Natively Probabilistic Computation
</div>
<div class="pub_author">
  Vikash Kumar Mansinghka, Ph.D. Thesis
</div>


<h2>Abstract:</h2>
<blockquote>
I introduce a new set of natively probabilistic computing abstractions, including probabilistic generalizations of Boolean circuits, backtracking search and pure Lisp. I show how these tools let one compactly specify probabilistic generative models, generalize and parallelize widely used sampling algorithms like rejection sampling and Markov chain Monte Carlo, and solve difficult Bayesian inference problems.<p>
I first introduce Church, a probabilistic programming language for describing probabilistic generative processes that induce distributions, which generalizes Lisp, a language for describing deterministic procedures that induce functions. I highlight the ways randomness meshes with the reflectiveness of Lisp to support the representation of structured, uncertain knowledge, including nonparametric Bayesian models from the current literature, programs for decision making under uncertainty, and programs that learn very simple programs from data. I then introduce systematic stochastic search, a recursive algorithm for exact and approximate sampling that generalizes a popular form of backtracking search to the broader setting of stochastic simulation and recovers widely used particle filters as a special case. I use it to solve probabilistic reasoning problems from statistical physics, causal reasoning and stereo vision. Finally, I introduce stochastic digital circuits that model the probability algebra just as traditional Boolean circuits model the Boolean algebra. I show how these circuits can be used to build massively parallel, fault-tolerant machines for sampling and allow one to efficiently run Markov chain Monte Carlo methods on models with hundreds of thousands of variables in real time.<p>
I emphasize the ways in which these ideas fit together into a coherent software and hardware stack for natively probabilistic computing, organized around distributions and samplers rather than deterministic functions. I argue that by building uncertainty and randomness into the foundations of our programming languages and computing machines, we may arrive at ones that are more powerful, flexible and efficient than deterministic designs, and are in better alignment with the needs of computational science, statistics and artificial intelligence.
</blockquote>

<h2>Published:</h2>
<blockquote>
  Natively Probabilistic Computation, Vikash Kumar Mansinghka.<br>
  <i>Ph.D Thesis</i>, Department of Brain and Cognitive Sciences, MIT, 2009.
</blockquote>

<h2>Download:</h2>
<ul>
  <li><a href="2009-06-MansinghkaThesis.pdf">Natively Probabilistic Computation</a> (PDF)</li>
</ul>

</body>
</html>
